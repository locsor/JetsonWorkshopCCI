{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a57f1ff2-25ea-4f77-b242-b1c0d3c4d358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, 1)\n",
    "        self.conv4 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.conv5 = nn.Conv2d(64, 64, 3, 1)\n",
    "        self.conv6 = nn.Conv2d(64, 128, 3, 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(4608, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, epoch, dry=False):\n",
    "    NUM_ACCUMULATION_STEPS = 8\n",
    "\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        \n",
    "        # with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss = loss / NUM_ACCUMULATION_STEPS\n",
    "        \n",
    "        loss.backward()\n",
    "        # if ((batch_idx + 1) % NUM_ACCUMULATION_STEPS == 0) or (batch_idx + 1 == len(train_loader)):\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # optimizer.step()\n",
    "        # optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "        if dry:\n",
    "            break\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            # test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            test_loss = criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "378b4bda-be36-4b1f-9c03-4cfbbb92028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=102):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer9 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer10 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer11 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer12 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer13 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(7*7*512, 1024),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU())\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(1024, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        # out = self.layer6(out)\n",
    "        out = self.layer7(out)\n",
    "        out = self.layer8(out)\n",
    "        # out = self.layer9(out)\n",
    "        out = self.layer10(out)\n",
    "        out = self.layer11(out)\n",
    "        # out = self.layer12(out)\n",
    "        # out = self.layer13(out)\n",
    "        # out = out.reshape(out.size(0), -1)\n",
    "        out = torch.flatten(out, 1)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7ff64bd-af72-4a45-98eb-c41e97cdbd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/1020 (0%)]\tLoss: 4.551723\n",
      "Train Epoch: 0 [0/1020 (0%)]\tLoss: 4.552531\n",
      "Train Epoch: 0 [400/1020 (39%)]\tLoss: 4.740227\n",
      "Train Epoch: 0 [800/1020 (78%)]\tLoss: 4.604862\n",
      "\n",
      "Test set: Average loss: 0.0044, Accuracy: 20/1020 (2%)\n",
      "\n",
      "Train Epoch: 1 [0/1020 (0%)]\tLoss: 4.063036\n",
      "Train Epoch: 1 [400/1020 (39%)]\tLoss: 4.327768\n",
      "Train Epoch: 1 [800/1020 (78%)]\tLoss: 4.179581\n",
      "\n",
      "Test set: Average loss: 0.0043, Accuracy: 40/1020 (4%)\n",
      "\n",
      "Train Epoch: 2 [0/1020 (0%)]\tLoss: 4.744296\n",
      "Train Epoch: 2 [400/1020 (39%)]\tLoss: 4.501762\n",
      "Train Epoch: 2 [800/1020 (78%)]\tLoss: 4.697492\n",
      "\n",
      "Test set: Average loss: 0.0040, Accuracy: 57/1020 (6%)\n",
      "\n",
      "Train Epoch: 3 [0/1020 (0%)]\tLoss: 4.311679\n",
      "Train Epoch: 3 [400/1020 (39%)]\tLoss: 4.245236\n",
      "Train Epoch: 3 [800/1020 (78%)]\tLoss: 4.400137\n",
      "\n",
      "Test set: Average loss: 0.0037, Accuracy: 57/1020 (6%)\n",
      "\n",
      "Train Epoch: 4 [0/1020 (0%)]\tLoss: 4.349289\n",
      "Train Epoch: 4 [400/1020 (39%)]\tLoss: 3.282009\n",
      "Train Epoch: 4 [800/1020 (78%)]\tLoss: 3.517829\n",
      "\n",
      "Test set: Average loss: 0.0042, Accuracy: 95/1020 (9%)\n",
      "\n",
      "73.59332995414734\n"
     ]
    }
   ],
   "source": [
    "train_kwargs = {'batch_size': 4}\n",
    "test_kwargs = {'batch_size': 4}\n",
    "\n",
    "accel_kwargs = {'num_workers': 1,\n",
    "                'persistent_workers': True,\n",
    "               # 'pin_memory': True,\n",
    "               'shuffle': True}\n",
    "\n",
    "train_kwargs.update(accel_kwargs)\n",
    "test_kwargs.update(accel_kwargs)\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    torchvision.models.VGG16_BN_Weights.IMAGENET1K_V1.transforms(),\n",
    "    # transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "dataset1 = datasets.Flowers102('./data', split=\"train\", download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.Flowers102('./data', split=\"val\", download=True,\n",
    "                   transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, drop_last =True, **test_kwargs)\n",
    "\n",
    "model = VGG16().cuda()\n",
    "# optimizer = optim.Adadelta(model.parameters(), lr=0.1)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "\n",
    "# train(model, train_loader, optimizer, epoch, True)\n",
    "epoch = 0\n",
    "\n",
    "train(model, train_loader, optimizer, epoch, dry=True)\n",
    "times = []\n",
    "\n",
    "for epoch in range(5):\n",
    "    torch.cuda.synchronize()\n",
    "    start_epoch = time.time()\n",
    "    \n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "        \n",
    "    test(model, test_loader)\n",
    "    end_epoch = time.time()\n",
    "    elapsed = end_epoch - start_epoch\n",
    "    times.append(elapsed)\n",
    "\n",
    "avg_time = sum(times)/len(times)\n",
    "print(avg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bc281c2-ad4d-4829-9861-7b5fc2d70b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/1020 (0%)]\tLoss: 4.749023\n",
      "Train Epoch: 0 [0/1020 (0%)]\tLoss: 4.249512\n",
      "Train Epoch: 0 [400/1020 (39%)]\tLoss: 4.958984\n",
      "Train Epoch: 0 [800/1020 (78%)]\tLoss: 4.659180\n",
      "\n",
      "Test set: Average loss: 0.0043, Accuracy: 31/1020 (3%)\n",
      "\n",
      "Train Epoch: 1 [0/1020 (0%)]\tLoss: 4.521484\n",
      "Train Epoch: 1 [400/1020 (39%)]\tLoss: 4.424805\n",
      "Train Epoch: 1 [800/1020 (78%)]\tLoss: 4.817383\n",
      "\n",
      "Test set: Average loss: 0.0044, Accuracy: 29/1020 (3%)\n",
      "\n",
      "Train Epoch: 2 [0/1020 (0%)]\tLoss: 4.669922\n",
      "Train Epoch: 2 [400/1020 (39%)]\tLoss: 4.071289\n",
      "Train Epoch: 2 [800/1020 (78%)]\tLoss: 4.270508\n",
      "\n",
      "Test set: Average loss: 0.0035, Accuracy: 36/1020 (4%)\n",
      "\n",
      "Train Epoch: 3 [0/1020 (0%)]\tLoss: 4.433105\n",
      "Train Epoch: 3 [400/1020 (39%)]\tLoss: 4.254883\n",
      "Train Epoch: 3 [800/1020 (78%)]\tLoss: 4.182129\n",
      "\n",
      "Test set: Average loss: 0.0039, Accuracy: 50/1020 (5%)\n",
      "\n",
      "Train Epoch: 4 [0/1020 (0%)]\tLoss: 4.062988\n",
      "Train Epoch: 4 [400/1020 (39%)]\tLoss: 3.959961\n",
      "Train Epoch: 4 [800/1020 (78%)]\tLoss: 4.550293\n",
      "\n",
      "Test set: Average loss: 0.0033, Accuracy: 86/1020 (8%)\n",
      "\n",
      "58.088413286209104\n"
     ]
    }
   ],
   "source": [
    "train_kwargs = {'batch_size': 4}\n",
    "test_kwargs = {'batch_size': 4}\n",
    "\n",
    "accel_kwargs = {'num_workers': 1,\n",
    "                'persistent_workers': True,\n",
    "               # 'pin_memory': True,\n",
    "               'shuffle': True}\n",
    "\n",
    "train_kwargs.update(accel_kwargs)\n",
    "test_kwargs.update(accel_kwargs)\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    torchvision.models.VGG16_BN_Weights.IMAGENET1K_V1.transforms(),\n",
    "    # transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "dataset1 = datasets.Flowers102('./data', split=\"train\", download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.Flowers102('./data', split=\"val\", download=True,\n",
    "                   transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, drop_last =True, **test_kwargs)\n",
    "\n",
    "model = VGG16().cuda()\n",
    "# optimizer = optim.Adadelta(model.parameters(), lr=0.1)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "\n",
    "# train(model, train_loader, optimizer, epoch, True)\n",
    "epoch = 0\n",
    "\n",
    "train(model, train_loader, optimizer, epoch, dry=True)\n",
    "times = []\n",
    "\n",
    "for epoch in range(5):\n",
    "    torch.cuda.synchronize()\n",
    "    start_epoch = time.time()\n",
    "    \n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "        \n",
    "    test(model, test_loader)\n",
    "    end_epoch = time.time()\n",
    "    elapsed = end_epoch - start_epoch\n",
    "    times.append(elapsed)\n",
    "\n",
    "avg_time = sum(times)/len(times)\n",
    "print(avg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90783091-5233-48a4-9421-d6e1013e015c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/1020 (0%)]\tLoss: 0.598389\n",
      "Train Epoch: 0 [0/1020 (0%)]\tLoss: 0.586914\n",
      "Train Epoch: 0 [400/1020 (39%)]\tLoss: 0.595581\n",
      "Train Epoch: 0 [800/1020 (78%)]\tLoss: 0.596313\n",
      "\n",
      "Test set: Average loss: 0.0045, Accuracy: 25/1020 (2%)\n",
      "\n",
      "Train Epoch: 1 [0/1020 (0%)]\tLoss: 0.597534\n",
      "Train Epoch: 1 [400/1020 (39%)]\tLoss: 0.558472\n",
      "Train Epoch: 1 [800/1020 (78%)]\tLoss: 0.586792\n",
      "\n",
      "Test set: Average loss: 0.0046, Accuracy: 42/1020 (4%)\n",
      "\n",
      "Train Epoch: 2 [0/1020 (0%)]\tLoss: 0.560913\n",
      "Train Epoch: 2 [400/1020 (39%)]\tLoss: 0.578125\n",
      "Train Epoch: 2 [800/1020 (78%)]\tLoss: 0.541992\n",
      "\n",
      "Test set: Average loss: 0.0044, Accuracy: 51/1020 (5%)\n",
      "\n",
      "Train Epoch: 3 [0/1020 (0%)]\tLoss: 0.549072\n",
      "Train Epoch: 3 [400/1020 (39%)]\tLoss: 0.571777\n",
      "Train Epoch: 3 [800/1020 (78%)]\tLoss: 0.536865\n",
      "\n",
      "Test set: Average loss: 0.0043, Accuracy: 55/1020 (5%)\n",
      "\n",
      "Train Epoch: 4 [0/1020 (0%)]\tLoss: 0.495178\n",
      "Train Epoch: 4 [400/1020 (39%)]\tLoss: 0.531738\n",
      "Train Epoch: 4 [800/1020 (78%)]\tLoss: 0.538452\n",
      "\n",
      "Test set: Average loss: 0.0040, Accuracy: 72/1020 (7%)\n",
      "\n",
      "Train Epoch: 5 [0/1020 (0%)]\tLoss: 0.487610\n",
      "Train Epoch: 5 [400/1020 (39%)]\tLoss: 0.540710\n",
      "Train Epoch: 5 [800/1020 (78%)]\tLoss: 0.547974\n",
      "\n",
      "Test set: Average loss: 0.0036, Accuracy: 99/1020 (10%)\n",
      "\n",
      "Train Epoch: 6 [0/1020 (0%)]\tLoss: 0.468079\n",
      "Train Epoch: 6 [400/1020 (39%)]\tLoss: 0.545288\n",
      "Train Epoch: 6 [800/1020 (78%)]\tLoss: 0.548157\n",
      "\n",
      "Test set: Average loss: 0.0037, Accuracy: 104/1020 (10%)\n",
      "\n",
      "Train Epoch: 7 [0/1020 (0%)]\tLoss: 0.464630\n",
      "Train Epoch: 7 [400/1020 (39%)]\tLoss: 0.487671\n",
      "Train Epoch: 7 [800/1020 (78%)]\tLoss: 0.497803\n",
      "\n",
      "Test set: Average loss: 0.0037, Accuracy: 123/1020 (12%)\n",
      "\n",
      "Train Epoch: 8 [0/1020 (0%)]\tLoss: 0.468506\n",
      "Train Epoch: 8 [400/1020 (39%)]\tLoss: 0.472168\n",
      "Train Epoch: 8 [800/1020 (78%)]\tLoss: 0.490112\n",
      "\n",
      "Test set: Average loss: 0.0030, Accuracy: 124/1020 (12%)\n",
      "\n",
      "Train Epoch: 9 [0/1020 (0%)]\tLoss: 0.515503\n",
      "Train Epoch: 9 [400/1020 (39%)]\tLoss: 0.377747\n",
      "Train Epoch: 9 [800/1020 (78%)]\tLoss: 0.384460\n",
      "\n",
      "Test set: Average loss: 0.0032, Accuracy: 173/1020 (17%)\n",
      "\n",
      "Train Epoch: 10 [0/1020 (0%)]\tLoss: 0.315277\n",
      "Train Epoch: 10 [400/1020 (39%)]\tLoss: 0.457703\n",
      "Train Epoch: 10 [800/1020 (78%)]\tLoss: 0.481628\n",
      "\n",
      "Test set: Average loss: 0.0048, Accuracy: 165/1020 (16%)\n",
      "\n",
      "Train Epoch: 11 [0/1020 (0%)]\tLoss: 0.458557\n",
      "Train Epoch: 11 [400/1020 (39%)]\tLoss: 0.330750\n",
      "Train Epoch: 11 [800/1020 (78%)]\tLoss: 0.276398\n",
      "\n",
      "Test set: Average loss: 0.0029, Accuracy: 199/1020 (20%)\n",
      "\n",
      "Train Epoch: 12 [0/1020 (0%)]\tLoss: 0.376038\n",
      "Train Epoch: 12 [400/1020 (39%)]\tLoss: 0.446533\n",
      "Train Epoch: 12 [800/1020 (78%)]\tLoss: 0.437256\n",
      "\n",
      "Test set: Average loss: 0.0041, Accuracy: 215/1020 (21%)\n",
      "\n",
      "Train Epoch: 13 [0/1020 (0%)]\tLoss: 0.579224\n",
      "Train Epoch: 13 [400/1020 (39%)]\tLoss: 0.372131\n",
      "Train Epoch: 13 [800/1020 (78%)]\tLoss: 0.348038\n",
      "\n",
      "Test set: Average loss: 0.0048, Accuracy: 238/1020 (23%)\n",
      "\n",
      "Train Epoch: 14 [0/1020 (0%)]\tLoss: 0.286285\n",
      "Train Epoch: 14 [400/1020 (39%)]\tLoss: 0.411316\n",
      "Train Epoch: 14 [800/1020 (78%)]\tLoss: 0.360199\n",
      "\n",
      "Test set: Average loss: 0.0042, Accuracy: 199/1020 (20%)\n",
      "\n",
      "Train Epoch: 15 [0/1020 (0%)]\tLoss: 0.261612\n",
      "Train Epoch: 15 [400/1020 (39%)]\tLoss: 0.419678\n",
      "Train Epoch: 15 [800/1020 (78%)]\tLoss: 0.388855\n",
      "\n",
      "Test set: Average loss: 0.0024, Accuracy: 255/1020 (25%)\n",
      "\n",
      "Train Epoch: 16 [0/1020 (0%)]\tLoss: 0.182617\n",
      "Train Epoch: 16 [400/1020 (39%)]\tLoss: 0.104385\n",
      "Train Epoch: 16 [800/1020 (78%)]\tLoss: 0.232941\n",
      "\n",
      "Test set: Average loss: 0.0024, Accuracy: 258/1020 (25%)\n",
      "\n",
      "Train Epoch: 17 [0/1020 (0%)]\tLoss: 0.233513\n",
      "Train Epoch: 17 [400/1020 (39%)]\tLoss: 0.164169\n",
      "Train Epoch: 17 [800/1020 (78%)]\tLoss: 0.133911\n",
      "\n",
      "Test set: Average loss: 0.0037, Accuracy: 232/1020 (23%)\n",
      "\n",
      "Train Epoch: 18 [0/1020 (0%)]\tLoss: 0.393555\n",
      "Train Epoch: 18 [400/1020 (39%)]\tLoss: 0.175110\n",
      "Train Epoch: 18 [800/1020 (78%)]\tLoss: 0.253754\n",
      "\n",
      "Test set: Average loss: 0.0021, Accuracy: 285/1020 (28%)\n",
      "\n",
      "Train Epoch: 19 [0/1020 (0%)]\tLoss: 0.206238\n",
      "Train Epoch: 19 [400/1020 (39%)]\tLoss: 0.189758\n",
      "Train Epoch: 19 [800/1020 (78%)]\tLoss: 0.187950\n",
      "\n",
      "Test set: Average loss: 0.0031, Accuracy: 279/1020 (27%)\n",
      "\n",
      "Train Epoch: 20 [0/1020 (0%)]\tLoss: 0.369171\n",
      "Train Epoch: 20 [400/1020 (39%)]\tLoss: 0.273239\n",
      "Train Epoch: 20 [800/1020 (78%)]\tLoss: 0.206299\n",
      "\n",
      "Test set: Average loss: 0.0031, Accuracy: 328/1020 (32%)\n",
      "\n",
      "Train Epoch: 21 [0/1020 (0%)]\tLoss: 0.227512\n",
      "Train Epoch: 21 [400/1020 (39%)]\tLoss: 0.291512\n",
      "Train Epoch: 21 [800/1020 (78%)]\tLoss: 0.121540\n",
      "\n",
      "Test set: Average loss: 0.0038, Accuracy: 272/1020 (27%)\n",
      "\n",
      "Train Epoch: 22 [0/1020 (0%)]\tLoss: 0.140611\n",
      "Train Epoch: 22 [400/1020 (39%)]\tLoss: 0.288177\n",
      "Train Epoch: 22 [800/1020 (78%)]\tLoss: 0.113312\n",
      "\n",
      "Test set: Average loss: 0.0008, Accuracy: 311/1020 (30%)\n",
      "\n",
      "Train Epoch: 23 [0/1020 (0%)]\tLoss: 0.130024\n",
      "Train Epoch: 23 [400/1020 (39%)]\tLoss: 0.191879\n",
      "Train Epoch: 23 [800/1020 (78%)]\tLoss: 0.120541\n",
      "\n",
      "Test set: Average loss: 0.0035, Accuracy: 317/1020 (31%)\n",
      "\n",
      "Train Epoch: 24 [0/1020 (0%)]\tLoss: 0.049936\n",
      "Train Epoch: 24 [400/1020 (39%)]\tLoss: 0.198074\n",
      "Train Epoch: 24 [800/1020 (78%)]\tLoss: 0.044678\n",
      "\n",
      "Test set: Average loss: 0.0033, Accuracy: 316/1020 (31%)\n",
      "\n",
      "Train Epoch: 25 [0/1020 (0%)]\tLoss: 0.136830\n",
      "Train Epoch: 25 [400/1020 (39%)]\tLoss: 0.129557\n",
      "Train Epoch: 25 [800/1020 (78%)]\tLoss: 0.107414\n",
      "\n",
      "Test set: Average loss: 0.0025, Accuracy: 325/1020 (32%)\n",
      "\n",
      "Train Epoch: 26 [0/1020 (0%)]\tLoss: 0.066728\n",
      "Train Epoch: 26 [400/1020 (39%)]\tLoss: 0.082808\n",
      "Train Epoch: 26 [800/1020 (78%)]\tLoss: 0.194504\n",
      "\n",
      "Test set: Average loss: 0.0017, Accuracy: 336/1020 (33%)\n",
      "\n",
      "Train Epoch: 27 [0/1020 (0%)]\tLoss: 0.042889\n",
      "Train Epoch: 27 [400/1020 (39%)]\tLoss: 0.024102\n",
      "Train Epoch: 27 [800/1020 (78%)]\tLoss: 0.214851\n",
      "\n",
      "Test set: Average loss: 0.0025, Accuracy: 311/1020 (30%)\n",
      "\n",
      "Train Epoch: 28 [0/1020 (0%)]\tLoss: 0.041990\n",
      "Train Epoch: 28 [400/1020 (39%)]\tLoss: 0.049583\n",
      "Train Epoch: 28 [800/1020 (78%)]\tLoss: 0.075706\n",
      "\n",
      "Test set: Average loss: 0.0047, Accuracy: 323/1020 (32%)\n",
      "\n",
      "Train Epoch: 29 [0/1020 (0%)]\tLoss: 0.060755\n",
      "Train Epoch: 29 [400/1020 (39%)]\tLoss: 0.042058\n",
      "Train Epoch: 29 [800/1020 (78%)]\tLoss: 0.117564\n",
      "\n",
      "Test set: Average loss: 0.0023, Accuracy: 357/1020 (35%)\n",
      "\n",
      "Train Epoch: 30 [0/1020 (0%)]\tLoss: 0.065128\n",
      "Train Epoch: 30 [400/1020 (39%)]\tLoss: 0.019167\n",
      "Train Epoch: 30 [800/1020 (78%)]\tLoss: 0.017394\n",
      "\n",
      "Test set: Average loss: 0.0029, Accuracy: 335/1020 (33%)\n",
      "\n",
      "Train Epoch: 31 [0/1020 (0%)]\tLoss: 0.028505\n",
      "Train Epoch: 31 [400/1020 (39%)]\tLoss: 0.011291\n",
      "Train Epoch: 31 [800/1020 (78%)]\tLoss: 0.045291\n",
      "\n",
      "Test set: Average loss: 0.0009, Accuracy: 368/1020 (36%)\n",
      "\n",
      "Train Epoch: 32 [0/1020 (0%)]\tLoss: 0.021721\n",
      "Train Epoch: 32 [400/1020 (39%)]\tLoss: 0.055986\n",
      "Train Epoch: 32 [800/1020 (78%)]\tLoss: 0.025814\n",
      "\n",
      "Test set: Average loss: 0.0026, Accuracy: 356/1020 (35%)\n",
      "\n",
      "Train Epoch: 33 [0/1020 (0%)]\tLoss: 0.048459\n",
      "Train Epoch: 33 [400/1020 (39%)]\tLoss: 0.006397\n",
      "Train Epoch: 33 [800/1020 (78%)]\tLoss: 0.049870\n",
      "\n",
      "Test set: Average loss: 0.0026, Accuracy: 361/1020 (35%)\n",
      "\n",
      "Train Epoch: 34 [0/1020 (0%)]\tLoss: 0.024962\n",
      "Train Epoch: 34 [400/1020 (39%)]\tLoss: 0.039432\n",
      "Train Epoch: 34 [800/1020 (78%)]\tLoss: 0.033670\n",
      "\n",
      "Test set: Average loss: 0.0030, Accuracy: 359/1020 (35%)\n",
      "\n",
      "Train Epoch: 35 [0/1020 (0%)]\tLoss: 0.037315\n",
      "Train Epoch: 35 [400/1020 (39%)]\tLoss: 0.012712\n",
      "Train Epoch: 35 [800/1020 (78%)]\tLoss: 0.006516\n",
      "\n",
      "Test set: Average loss: 0.0033, Accuracy: 359/1020 (35%)\n",
      "\n",
      "Train Epoch: 36 [0/1020 (0%)]\tLoss: 0.005317\n",
      "Train Epoch: 36 [400/1020 (39%)]\tLoss: 0.001788\n",
      "Train Epoch: 36 [800/1020 (78%)]\tLoss: 0.039007\n",
      "\n",
      "Test set: Average loss: 0.0043, Accuracy: 378/1020 (37%)\n",
      "\n",
      "Train Epoch: 37 [0/1020 (0%)]\tLoss: 0.013001\n",
      "Train Epoch: 37 [400/1020 (39%)]\tLoss: 0.005178\n",
      "Train Epoch: 37 [800/1020 (78%)]\tLoss: 0.007987\n",
      "\n",
      "Test set: Average loss: 0.0030, Accuracy: 377/1020 (37%)\n",
      "\n",
      "Train Epoch: 38 [0/1020 (0%)]\tLoss: 0.013052\n",
      "Train Epoch: 38 [400/1020 (39%)]\tLoss: 0.011464\n",
      "Train Epoch: 38 [800/1020 (78%)]\tLoss: 0.008078\n",
      "\n",
      "Test set: Average loss: 0.0019, Accuracy: 360/1020 (35%)\n",
      "\n",
      "Train Epoch: 39 [0/1020 (0%)]\tLoss: 0.069748\n",
      "Train Epoch: 39 [400/1020 (39%)]\tLoss: 0.007743\n",
      "Train Epoch: 39 [800/1020 (78%)]\tLoss: 0.017114\n",
      "\n",
      "Test set: Average loss: 0.0045, Accuracy: 366/1020 (36%)\n",
      "\n",
      "Train Epoch: 40 [0/1020 (0%)]\tLoss: 0.001015\n",
      "Train Epoch: 40 [400/1020 (39%)]\tLoss: 0.013402\n",
      "Train Epoch: 40 [800/1020 (78%)]\tLoss: 0.005455\n",
      "\n",
      "Test set: Average loss: 0.0025, Accuracy: 362/1020 (35%)\n",
      "\n",
      "Train Epoch: 41 [0/1020 (0%)]\tLoss: 0.016812\n",
      "Train Epoch: 41 [400/1020 (39%)]\tLoss: 0.055028\n",
      "Train Epoch: 41 [800/1020 (78%)]\tLoss: 0.046995\n",
      "\n",
      "Test set: Average loss: 0.0035, Accuracy: 346/1020 (34%)\n",
      "\n",
      "Train Epoch: 42 [0/1020 (0%)]\tLoss: 0.038513\n",
      "Train Epoch: 42 [400/1020 (39%)]\tLoss: 0.001087\n",
      "Train Epoch: 42 [800/1020 (78%)]\tLoss: 0.003922\n",
      "\n",
      "Test set: Average loss: 0.0024, Accuracy: 372/1020 (36%)\n",
      "\n",
      "Train Epoch: 43 [0/1020 (0%)]\tLoss: 0.002686\n",
      "Train Epoch: 43 [400/1020 (39%)]\tLoss: 0.034678\n",
      "Train Epoch: 43 [800/1020 (78%)]\tLoss: 0.050555\n",
      "\n",
      "Test set: Average loss: 0.0030, Accuracy: 372/1020 (36%)\n",
      "\n",
      "Train Epoch: 44 [0/1020 (0%)]\tLoss: 0.001354\n",
      "Train Epoch: 44 [400/1020 (39%)]\tLoss: 0.005714\n",
      "Train Epoch: 44 [800/1020 (78%)]\tLoss: 0.001792\n",
      "\n",
      "Test set: Average loss: 0.0031, Accuracy: 383/1020 (38%)\n",
      "\n",
      "Train Epoch: 45 [0/1020 (0%)]\tLoss: 0.003282\n",
      "Train Epoch: 45 [400/1020 (39%)]\tLoss: 0.006495\n",
      "Train Epoch: 45 [800/1020 (78%)]\tLoss: 0.110779\n",
      "\n",
      "Test set: Average loss: 0.0025, Accuracy: 376/1020 (37%)\n",
      "\n",
      "Train Epoch: 46 [0/1020 (0%)]\tLoss: 0.000514\n",
      "Train Epoch: 46 [400/1020 (39%)]\tLoss: 0.001454\n",
      "Train Epoch: 46 [800/1020 (78%)]\tLoss: 0.002252\n",
      "\n",
      "Test set: Average loss: 0.0067, Accuracy: 384/1020 (38%)\n",
      "\n",
      "Train Epoch: 47 [0/1020 (0%)]\tLoss: 0.030544\n",
      "Train Epoch: 47 [400/1020 (39%)]\tLoss: 0.012098\n",
      "Train Epoch: 47 [800/1020 (78%)]\tLoss: 0.002899\n",
      "\n",
      "Test set: Average loss: 0.0005, Accuracy: 394/1020 (39%)\n",
      "\n",
      "Train Epoch: 48 [0/1020 (0%)]\tLoss: 0.003616\n",
      "Train Epoch: 48 [400/1020 (39%)]\tLoss: 0.006265\n",
      "Train Epoch: 48 [800/1020 (78%)]\tLoss: 0.071683\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m     41\u001b[0m start_epoch \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 43\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m test(model, test_loader)\n\u001b[1;32m     46\u001b[0m end_epoch \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[17], line 77\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, epoch, dry)\u001b[0m\n\u001b[1;32m     75\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m---> 77\u001b[0m     data, target \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, target\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16):\n\u001b[1;32m     81\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_kwargs = {'batch_size': 4}\n",
    "test_kwargs = {'batch_size': 4}\n",
    "\n",
    "accel_kwargs = {'num_workers': 1,\n",
    "                'persistent_workers': True,\n",
    "               # 'pin_memory': True,\n",
    "               'shuffle': True}\n",
    "\n",
    "train_kwargs.update(accel_kwargs)\n",
    "test_kwargs.update(accel_kwargs)\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    torchvision.models.VGG16_BN_Weights.IMAGENET1K_V1.transforms(),\n",
    "    # transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "dataset1 = datasets.Flowers102('./data', split=\"train\", download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.Flowers102('./data', split=\"val\", download=True,\n",
    "                   transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, drop_last =True, **test_kwargs)\n",
    "\n",
    "model = VGG16().cuda()\n",
    "# optimizer = optim.Adadelta(model.parameters(), lr=0.1)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "\n",
    "# train(model, train_loader, optimizer, epoch, True)\n",
    "epoch = 0\n",
    "\n",
    "train(model, train_loader, optimizer, epoch, dry=True)\n",
    "times = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    torch.cuda.synchronize()\n",
    "    start_epoch = time.time()\n",
    "    \n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "        \n",
    "    test(model, test_loader)\n",
    "    end_epoch = time.time()\n",
    "    elapsed = end_epoch - start_epoch\n",
    "    times.append(elapsed)\n",
    "\n",
    "avg_time = sum(times)/len(times)\n",
    "print(avg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80027bb5-76f3-43b8-ac24-d8e4942f725c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n",
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n",
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n",
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n",
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n",
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/1020 (0%)]\tLoss: 0.575303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n",
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/1020 (0%)]\tLoss: 0.584045\n",
      "Train Epoch: 0 [400/1020 (39%)]\tLoss: 0.583655\n",
      "Train Epoch: 0 [800/1020 (78%)]\tLoss: 0.599123\n",
      "\n",
      "Test set: Average loss: 0.0046, Accuracy: 27/1020 (3%)\n",
      "\n",
      "Train Epoch: 1 [0/1020 (0%)]\tLoss: 0.573221\n",
      "Train Epoch: 1 [400/1020 (39%)]\tLoss: 0.577363\n"
     ]
    }
   ],
   "source": [
    "train_kwargs = {'batch_size': 4}\n",
    "test_kwargs = {'batch_size': 4}\n",
    "\n",
    "accel_kwargs = {'num_workers': 1,\n",
    "                'persistent_workers': True,\n",
    "               # 'pin_memory': True,\n",
    "               'shuffle': True}\n",
    "\n",
    "train_kwargs.update(accel_kwargs)\n",
    "test_kwargs.update(accel_kwargs)\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    torchvision.models.VGG16_BN_Weights.IMAGENET1K_V1.transforms(),\n",
    "    # transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "dataset1 = datasets.Flowers102('./data', split=\"train\", download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.Flowers102('./data', split=\"val\", download=True,\n",
    "                   transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, drop_last =True, **test_kwargs)\n",
    "\n",
    "model = VGG16().cuda()\n",
    "# optimizer = optim.Adadelta(model.parameters(), lr=0.1)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "\n",
    "# train(model, train_loader, optimizer, epoch, True)\n",
    "epoch = 0\n",
    "\n",
    "train(model, train_loader, optimizer, epoch, dry=True)\n",
    "times = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    torch.cuda.synchronize()\n",
    "    start_epoch = time.time()\n",
    "    \n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "        \n",
    "    test(model, test_loader)\n",
    "    end_epoch = time.time()\n",
    "    elapsed = end_epoch - start_epoch\n",
    "    times.append(elapsed)\n",
    "\n",
    "avg_time = sum(times)/len(times)\n",
    "print(avg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f0b8f3-f73c-4aff-adb5-7724a238f2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

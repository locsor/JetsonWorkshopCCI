{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53295b45-db6b-437a-85f9-3683216b92a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, epoch, dry=False):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "        if dry:\n",
    "            break\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a36c35c6-0902-46dc-a4f8-d661c0248008",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.332272\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.167748\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.105755\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.123772\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.043670\n",
      "\n",
      "Test set: Average loss: 0.0551, Accuracy: 9801/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0551, Accuracy: 9801/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0551, Accuracy: 9801/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0551, Accuracy: 9801/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0551, Accuracy: 9801/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0551, Accuracy: 9801/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0551, Accuracy: 9801/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0551, Accuracy: 9801/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0550, Accuracy: 9801/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0550, Accuracy: 9801/10000 (98%)\n",
      "\n",
      "3.2278727293014526\n"
     ]
    }
   ],
   "source": [
    "train_kwargs = {'batch_size': 128}\n",
    "test_kwargs = {'batch_size': 128}\n",
    "\n",
    "accel_kwargs = {'num_workers': 1,\n",
    "                'persistent_workers': True,\n",
    "               'pin_memory': True,\n",
    "               'shuffle': True}\n",
    "train_kwargs.update(accel_kwargs)\n",
    "test_kwargs.update(accel_kwargs)\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "dataset1 = datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.MNIST('./data', train=False,\n",
    "                   transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, drop_last =True, **test_kwargs)\n",
    "\n",
    "model = Net().cuda()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.1)\n",
    "epoch = 0\n",
    "\n",
    "train(model, train_loader, optimizer, epoch)\n",
    "times = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    torch.cuda.synchronize()\n",
    "    start_epoch = time.time()\n",
    "    test(model, test_loader)\n",
    "    end_epoch = time.time()\n",
    "    elapsed = end_epoch - start_epoch\n",
    "    times.append(elapsed)\n",
    "\n",
    "avg_time = sum(times)/len(times)\n",
    "print(avg_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee23f88e-9fea-4366-b415-21528decbefb",
   "metadata": {},
   "source": [
    "<h1>Fused Part</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bca8960-e9fd-4ce2-bb39-3bc1375497f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if Fussed layers are valid: True\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd.function import once_differentiable\n",
    "\n",
    "def convolution_backward(grad_out, X, weight):\n",
    "    grad_input = F.conv2d(X.transpose(0, 1), grad_out.transpose(0, 1)).transpose(0, 1)\n",
    "    grad_X = F.conv_transpose2d(grad_out, weight)\n",
    "    return grad_X, grad_input\n",
    "\n",
    "class Conv2D(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, weight):\n",
    "        ctx.save_for_backward(X, weight)\n",
    "        return F.conv2d(X, weight)\n",
    "\n",
    "    # Use @once_differentiable by default unless we intend to double backward\n",
    "    @staticmethod\n",
    "    @once_differentiable\n",
    "    def backward(ctx, grad_out):\n",
    "        X, weight = ctx.saved_tensors\n",
    "        return convolution_backward(grad_out, X, weight)\n",
    "\n",
    "def unsqueeze_all(t):\n",
    "    # Helper function to ``unsqueeze`` all the dimensions that we reduce over\n",
    "    return t[None, :, None, None]\n",
    "\n",
    "def batch_norm_backward(grad_out, X, sum, sqrt_var, N, eps):\n",
    "    # We use the formula: ``out = (X - mean(X)) / (sqrt(var(X)) + eps)``\n",
    "    # in batch norm 2D forward. To simplify our derivation, we follow the\n",
    "    # chain rule and compute the gradients as follows before accumulating\n",
    "    # them all into a final grad_input.\n",
    "    #  1) ``grad of out wrt var(X)`` * ``grad of var(X) wrt X``\n",
    "    #  2) ``grad of out wrt mean(X)`` * ``grad of mean(X) wrt X``\n",
    "    #  3) ``grad of out wrt X in the numerator`` * ``grad of X wrt X``\n",
    "    # We then rewrite the formulas to use as few extra buffers as possible\n",
    "    tmp = ((X - unsqueeze_all(sum) / N) * grad_out).sum(dim=(0, 2, 3))\n",
    "    tmp *= -1\n",
    "    d_denom = tmp / (sqrt_var + eps)**2  # ``d_denom = -num / denom**2``\n",
    "    # It is useful to delete tensors when you no longer need them with ``del``\n",
    "    # For example, we could've done ``del tmp`` here because we won't use it later\n",
    "    # In this case, it's not a big difference because ``tmp`` only has size of (C,)\n",
    "    # The important thing is avoid allocating NCHW-sized tensors unnecessarily\n",
    "    d_var = d_denom / (2 * sqrt_var)  # ``denom = torch.sqrt(var) + eps``\n",
    "    # Compute ``d_mean_dx`` before allocating the final NCHW-sized grad_input buffer\n",
    "    d_mean_dx = grad_out / unsqueeze_all(sqrt_var + eps)\n",
    "    d_mean_dx = unsqueeze_all(-d_mean_dx.sum(dim=(0, 2, 3)) / N)\n",
    "    # ``d_mean_dx`` has already been reassigned to a C-sized buffer so no need to worry\n",
    "\n",
    "    # ``(1) unbiased_var(x) = ((X - unsqueeze_all(mean))**2).sum(dim=(0, 2, 3)) / (N - 1)``\n",
    "    grad_input = X * unsqueeze_all(d_var * N)\n",
    "    grad_input += unsqueeze_all(-d_var * sum)\n",
    "    grad_input *= 2 / ((N - 1) * N)\n",
    "    # (2) mean (see above)\n",
    "    grad_input += d_mean_dx\n",
    "    # (3) Add 'grad_out / <factor>' without allocating an extra buffer\n",
    "    grad_input *= unsqueeze_all(sqrt_var + eps)\n",
    "    grad_input += grad_out\n",
    "    grad_input /= unsqueeze_all(sqrt_var + eps)  # ``sqrt_var + eps > 0!``\n",
    "    return grad_input\n",
    "\n",
    "class BatchNorm(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, eps=1e-3):\n",
    "        # Don't save ``keepdim`` values for backward\n",
    "        sum = X.sum(dim=(0, 2, 3))\n",
    "        var = X.var(unbiased=True, dim=(0, 2, 3))\n",
    "        N = X.numel() / X.size(1)\n",
    "        sqrt_var = torch.sqrt(var)\n",
    "        ctx.save_for_backward(X)\n",
    "        ctx.eps = eps\n",
    "        ctx.sum = sum\n",
    "        ctx.N = N\n",
    "        ctx.sqrt_var = sqrt_var\n",
    "        mean = sum / N\n",
    "        denom = sqrt_var + eps\n",
    "        out = X - unsqueeze_all(mean)\n",
    "        out /= unsqueeze_all(denom)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    @once_differentiable\n",
    "    def backward(ctx, grad_out):\n",
    "        X, = ctx.saved_tensors\n",
    "        return batch_norm_backward(grad_out, X, ctx.sum, ctx.sqrt_var, ctx.N, ctx.eps)\n",
    "\n",
    "class FusedConvBN2DFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, conv_weight, eps=1e-3):\n",
    "        assert X.ndim == 4  # N, C, H, W\n",
    "        # (1) Only need to save this single buffer for backward!\n",
    "        ctx.save_for_backward(X, conv_weight)\n",
    "\n",
    "        # (2) Exact same Conv2D forward from example above\n",
    "        X = F.conv2d(X, conv_weight)\n",
    "        # (3) Exact same BatchNorm2D forward from example above\n",
    "        sum = X.sum(dim=(0, 2, 3))\n",
    "        var = X.var(unbiased=True, dim=(0, 2, 3))\n",
    "        N = X.numel() / X.size(1)\n",
    "        sqrt_var = torch.sqrt(var)\n",
    "        ctx.eps = eps\n",
    "        ctx.sum = sum\n",
    "        ctx.N = N\n",
    "        ctx.sqrt_var = sqrt_var\n",
    "        mean = sum / N\n",
    "        denom = sqrt_var + eps\n",
    "        # Try to do as many things in-place as possible\n",
    "        # Instead of `out = (X - a) / b`, doing `out = X - a; out /= b`\n",
    "        # avoids allocating one extra NCHW-sized buffer here\n",
    "        out = X - unsqueeze_all(mean)\n",
    "        out /= unsqueeze_all(denom)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        X, conv_weight, = ctx.saved_tensors\n",
    "        # (4) Batch norm backward\n",
    "        # (5) We need to recompute conv\n",
    "        X_conv_out = F.conv2d(X, conv_weight)\n",
    "        grad_out = batch_norm_backward(grad_out, X_conv_out, ctx.sum, ctx.sqrt_var,\n",
    "                                       ctx.N, ctx.eps)\n",
    "        # (6) Conv2d backward\n",
    "        grad_X, grad_input = convolution_backward(grad_out, X, conv_weight)\n",
    "        return grad_X, grad_input, None, None, None, None, None\n",
    "\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class FusedConvBN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, exp_avg_factor=0.1,\n",
    "                 eps=1e-3, device=None, dtype=None):\n",
    "        super(FusedConvBN, self).__init__()\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        # Conv parameters\n",
    "        weight_shape = (out_channels, in_channels, kernel_size, kernel_size)\n",
    "        self.conv_weight = nn.Parameter(torch.empty(*weight_shape, **factory_kwargs))\n",
    "        # Batch norm parameters\n",
    "        num_features = out_channels\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        # Initialize\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return FusedConvBN2DFunction.apply(X, self.conv_weight, self.eps)\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.kaiming_uniform_(self.conv_weight, a=math.sqrt(5))\n",
    "\n",
    "weight = torch.rand(5, 3, 3, 3, requires_grad=True, dtype=torch.double)\n",
    "X = torch.rand(2, 3, 4, 4, requires_grad=True, dtype=torch.double)\n",
    "print('Check if Fussed layers are valid:', torch.autograd.gradcheck(FusedConvBN2DFunction.apply, (X, weight)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "928f13b9-50d1-462a-8dd6-35a8a875724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetFused(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetFused, self).__init__()\n",
    "        self.convbn1 = FusedConvBN(1, 32, 3)\n",
    "        self.convbn2 = FusedConvBN(32, 64, 3)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convbn1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.convbn2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "032d7b02-9711-4f4d-8edf-80b52edab018",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.342265\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.209080\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.140363\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.090869\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.096224\n",
      "\n",
      "Test set: Average loss: 0.0591, Accuracy: 9792/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0589, Accuracy: 9790/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0588, Accuracy: 9795/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0586, Accuracy: 9795/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0591, Accuracy: 9798/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0582, Accuracy: 9796/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0584, Accuracy: 9800/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0582, Accuracy: 9798/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0587, Accuracy: 9791/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0590, Accuracy: 9793/10000 (98%)\n",
      "\n",
      "2.5884713888168336\n"
     ]
    }
   ],
   "source": [
    "model = NetFused().cuda()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.1)\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "train(model, train_loader, optimizer, epoch)\n",
    "times = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    torch.cuda.synchronize()\n",
    "    start_epoch = time.time()\n",
    "    test(model, test_loader)\n",
    "    end_epoch = time.time()\n",
    "    elapsed = end_epoch - start_epoch\n",
    "    times.append(elapsed)\n",
    "\n",
    "avg_time = sum(times)/len(times)\n",
    "print(avg_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4fe53b-4efe-425b-8fcf-6fe79016997e",
   "metadata": {},
   "source": [
    "<h1>Onnx Part</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43769b00-948a-4702-a260-af4d4ef6e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.device(\"cuda\"):\n",
    "    example_inputs = (torch.randn(128, 1, 28, 28),)\n",
    "    onnx_program = torch.onnx.export(model, example_inputs, \"model.onnx\", input_names=[\"x\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af416f12-a0b1-4ef6-99ad-72169f5fa9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.554471492767334\n",
      "1.4571020603179932\n",
      "1.455916166305542\n",
      "1.4561893939971924\n",
      "1.4564645290374756\n",
      "Average Time 1.4760287284851075\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "import tensorrt\n",
    "\n",
    "session = onnxruntime.InferenceSession(\"model.onnx\", providers=['TensorrtExecutionProvider','CUDAExecutionProvider'])\n",
    "X = torch.stack([d[0] for d in dataset2]).numpy()\n",
    "\n",
    "times = []\n",
    "\n",
    "for epoch in range(5):\n",
    "    start_epoch = time.time()\n",
    "    test_dataset = iter(test_loader)\n",
    "    for i in range((len(X)//128)-1):\n",
    "        # data = np.float32(next(test_dataset)[0])\n",
    "        outputs = session.run([], {'x':X[128*i:128*(i+1)]})[0]\n",
    "        \n",
    "    end_epoch = time.time()\n",
    "    times += [end_epoch - start_epoch]\n",
    "    print(times[-1])\n",
    "\n",
    "print('Average Time', sum(times)/len(times))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d45b7d-fb18-4eac-9f8e-d210c651b171",
   "metadata": {},
   "source": [
    "<h1>Prunning Part without Fused Layer</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51b82620-734c-47b5-9d04-84172163fb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.311606\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.210501\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.072079\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.099681\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.058206\n",
      "\n",
      "Test set: Average loss: 0.0574, Accuracy: 9805/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0571, Accuracy: 9807/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0574, Accuracy: 9805/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0572, Accuracy: 9806/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0573, Accuracy: 9805/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0573, Accuracy: 9806/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0573, Accuracy: 9805/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0574, Accuracy: 9805/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0573, Accuracy: 9805/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0574, Accuracy: 9805/10000 (98%)\n",
      "\n",
      "2.670937514305115\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "model = Net().cuda()\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (model.conv1, 'weight'),\n",
    "    (model.conv2, 'weight'),\n",
    "    (model.fc1, 'weight'),\n",
    "    (model.fc2, 'weight')\n",
    ")\n",
    "\n",
    "prune.ln_structured(model.conv1, name='weight', amount=0.2, n=2, dim=0)\n",
    "prune.ln_structured(model.conv2, name='weight', amount=0.2, n=2, dim=0)\n",
    "prune.ln_structured(model.fc1, name='weight', amount=0.2, n=2, dim=0)\n",
    "prune.ln_structured(model.fc2, name='weight', amount=0.2, n=2, dim=0)\n",
    "\n",
    "for module, name in parameters_to_prune:\n",
    "    prune.remove(module, name)\n",
    "    \n",
    "\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.1)\n",
    "epoch = 0\n",
    "\n",
    "train(model, train_loader, optimizer, epoch)\n",
    "times = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    torch.cuda.synchronize()\n",
    "    start_epoch = time.time()\n",
    "    test(model, test_loader)\n",
    "    end_epoch = time.time()\n",
    "    elapsed = end_epoch - start_epoch\n",
    "    times.append(elapsed)\n",
    "\n",
    "avg_time = sum(times)/len(times)\n",
    "print(avg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a8f2710-9934-4436-9f86-466a08176617",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.device(\"cuda\"):\n",
    "    example_inputs = (torch.randn(128, 1, 28, 28),)\n",
    "    onnx_program = torch.onnx.export(model, example_inputs, \"model.onnx\", input_names=[\"x\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cc75289-f1de-4ac0-8a37-35537a409004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8211748600006104\n",
      "0.6404070854187012\n",
      "0.6317508220672607\n",
      "0.631899356842041\n",
      "0.6345264911651611\n",
      "0.6719517230987548\n"
     ]
    }
   ],
   "source": [
    "session = onnxruntime.InferenceSession(\"model.onnx\", providers=['TensorrtExecutionProvider','CUDAExecutionProvider'])\n",
    "X = torch.stack([d[0] for d in dataset2]).numpy()\n",
    "\n",
    "times = []\n",
    "\n",
    "for epoch in range(5):\n",
    "    start_epoch = time.time()\n",
    "    test_dataset = iter(test_loader)\n",
    "    for i in range((len(X)//128)-1):\n",
    "        # data = np.float32(next(test_dataset)[0])\n",
    "        outputs = session.run([], {'x':X[128*i:128*(i+1)]})[0]\n",
    "        \n",
    "    end_epoch = time.time()\n",
    "    times += [end_epoch - start_epoch]\n",
    "    print(times[-1])\n",
    "\n",
    "print(sum(times)/len(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e69eca-95cb-4985-965b-d1ab14434899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

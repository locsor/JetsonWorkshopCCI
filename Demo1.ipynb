{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53295b45-db6b-437a-85f9-3683216b92a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        # self.convbn1 = FusedConvBN(1, 32, 3)\n",
    "        # self.convbn2 = FusedConvBN(32, 64, 3)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        # x = self.convbn1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        # x = self.convbn2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, epoch, dry=False):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "        if dry:\n",
    "            break\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0168c64-121a-4782-8200-f800bc1ad076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.function import once_differentiable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def convolution_backward(grad_out, X, weight):\n",
    "    grad_input = F.conv2d(X.transpose(0, 1), grad_out.transpose(0, 1)).transpose(0, 1)\n",
    "    grad_X = F.conv_transpose2d(grad_out, weight)\n",
    "    return grad_X, grad_input\n",
    "\n",
    "class Conv2D(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, weight):\n",
    "        ctx.save_for_backward(X, weight)\n",
    "        return F.conv2d(X, weight)\n",
    "\n",
    "    # Use @once_differentiable by default unless we intend to double backward\n",
    "    @staticmethod\n",
    "    @once_differentiable\n",
    "    def backward(ctx, grad_out):\n",
    "        X, weight = ctx.saved_tensors\n",
    "        return convolution_backward(grad_out, X, weight)\n",
    "\n",
    "def unsqueeze_all(t):\n",
    "    # Helper function to ``unsqueeze`` all the dimensions that we reduce over\n",
    "    return t[None, :, None, None]\n",
    "\n",
    "def batch_norm_backward(grad_out, X, sum, sqrt_var, N, eps):\n",
    "    # We use the formula: ``out = (X - mean(X)) / (sqrt(var(X)) + eps)``\n",
    "    # in batch norm 2D forward. To simplify our derivation, we follow the\n",
    "    # chain rule and compute the gradients as follows before accumulating\n",
    "    # them all into a final grad_input.\n",
    "    #  1) ``grad of out wrt var(X)`` * ``grad of var(X) wrt X``\n",
    "    #  2) ``grad of out wrt mean(X)`` * ``grad of mean(X) wrt X``\n",
    "    #  3) ``grad of out wrt X in the numerator`` * ``grad of X wrt X``\n",
    "    # We then rewrite the formulas to use as few extra buffers as possible\n",
    "    tmp = ((X - unsqueeze_all(sum) / N) * grad_out).sum(dim=(0, 2, 3))\n",
    "    tmp *= -1\n",
    "    d_denom = tmp / (sqrt_var + eps)**2  # ``d_denom = -num / denom**2``\n",
    "    # It is useful to delete tensors when you no longer need them with ``del``\n",
    "    # For example, we could've done ``del tmp`` here because we won't use it later\n",
    "    # In this case, it's not a big difference because ``tmp`` only has size of (C,)\n",
    "    # The important thing is avoid allocating NCHW-sized tensors unnecessarily\n",
    "    d_var = d_denom / (2 * sqrt_var)  # ``denom = torch.sqrt(var) + eps``\n",
    "    # Compute ``d_mean_dx`` before allocating the final NCHW-sized grad_input buffer\n",
    "    d_mean_dx = grad_out / unsqueeze_all(sqrt_var + eps)\n",
    "    d_mean_dx = unsqueeze_all(-d_mean_dx.sum(dim=(0, 2, 3)) / N)\n",
    "    # ``d_mean_dx`` has already been reassigned to a C-sized buffer so no need to worry\n",
    "\n",
    "    # ``(1) unbiased_var(x) = ((X - unsqueeze_all(mean))**2).sum(dim=(0, 2, 3)) / (N - 1)``\n",
    "    grad_input = X * unsqueeze_all(d_var * N)\n",
    "    grad_input += unsqueeze_all(-d_var * sum)\n",
    "    grad_input *= 2 / ((N - 1) * N)\n",
    "    # (2) mean (see above)\n",
    "    grad_input += d_mean_dx\n",
    "    # (3) Add 'grad_out / <factor>' without allocating an extra buffer\n",
    "    grad_input *= unsqueeze_all(sqrt_var + eps)\n",
    "    grad_input += grad_out\n",
    "    grad_input /= unsqueeze_all(sqrt_var + eps)  # ``sqrt_var + eps > 0!``\n",
    "    return grad_input\n",
    "\n",
    "class BatchNorm(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, eps=1e-3):\n",
    "        # Don't save ``keepdim`` values for backward\n",
    "        sum = X.sum(dim=(0, 2, 3))\n",
    "        var = X.var(unbiased=True, dim=(0, 2, 3))\n",
    "        N = X.numel() / X.size(1)\n",
    "        sqrt_var = torch.sqrt(var)\n",
    "        ctx.save_for_backward(X)\n",
    "        ctx.eps = eps\n",
    "        ctx.sum = sum\n",
    "        ctx.N = N\n",
    "        ctx.sqrt_var = sqrt_var\n",
    "        mean = sum / N\n",
    "        denom = sqrt_var + eps\n",
    "        out = X - unsqueeze_all(mean)\n",
    "        out /= unsqueeze_all(denom)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    @once_differentiable\n",
    "    def backward(ctx, grad_out):\n",
    "        X, = ctx.saved_tensors\n",
    "        return batch_norm_backward(grad_out, X, ctx.sum, ctx.sqrt_var, ctx.N, ctx.eps)\n",
    "\n",
    "class FusedConvBN2DFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, conv_weight, eps=1e-3):\n",
    "        assert X.ndim == 4  # N, C, H, W\n",
    "        # (1) Only need to save this single buffer for backward!\n",
    "        ctx.save_for_backward(X, conv_weight)\n",
    "\n",
    "        # (2) Exact same Conv2D forward from example above\n",
    "        X = F.conv2d(X, conv_weight)\n",
    "        # (3) Exact same BatchNorm2D forward from example above\n",
    "        sum = X.sum(dim=(0, 2, 3))\n",
    "        var = X.var(unbiased=True, dim=(0, 2, 3))\n",
    "        N = X.numel() / X.size(1)\n",
    "        sqrt_var = torch.sqrt(var)\n",
    "        ctx.eps = eps\n",
    "        ctx.sum = sum\n",
    "        ctx.N = N\n",
    "        ctx.sqrt_var = sqrt_var\n",
    "        mean = sum / N\n",
    "        denom = sqrt_var + eps\n",
    "        # Try to do as many things in-place as possible\n",
    "        # Instead of `out = (X - a) / b`, doing `out = X - a; out /= b`\n",
    "        # avoids allocating one extra NCHW-sized buffer here\n",
    "        out = X - unsqueeze_all(mean)\n",
    "        out /= unsqueeze_all(denom)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        X, conv_weight, = ctx.saved_tensors\n",
    "        # (4) Batch norm backward\n",
    "        # (5) We need to recompute conv\n",
    "        X_conv_out = F.conv2d(X, conv_weight)\n",
    "        grad_out = batch_norm_backward(grad_out, X_conv_out, ctx.sum, ctx.sqrt_var,\n",
    "                                       ctx.N, ctx.eps)\n",
    "        # (6) Conv2d backward\n",
    "        grad_X, grad_input = convolution_backward(grad_out, X, conv_weight)\n",
    "        return grad_X, grad_input, None, None, None, None, None\n",
    "\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class FusedConvBN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, exp_avg_factor=0.1,\n",
    "                 eps=1e-3, device=None, dtype=None):\n",
    "        super(FusedConvBN, self).__init__()\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        # Conv parameters\n",
    "        weight_shape = (out_channels, in_channels, kernel_size, kernel_size)\n",
    "        self.conv_weight = nn.Parameter(torch.empty(*weight_shape, **factory_kwargs))\n",
    "        # Batch norm parameters\n",
    "        num_features = out_channels\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        # Initialize\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return FusedConvBN2DFunction.apply(X, self.conv_weight, self.eps)\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.kaiming_uniform_(self.conv_weight, a=math.sqrt(5))\n",
    "\n",
    "# a = torch.rand(1, 2, 3, 4, requires_grad=True, dtype=torch.double)\n",
    "# torch.autograd.gradcheck(BatchNorm.apply, (a,), fast_mode=False)\n",
    "\n",
    "# weight = torch.rand(5, 3, 3, 3, requires_grad=True, dtype=torch.double)\n",
    "# X = torch.rand(10, 3, 7, 7, requires_grad=True, dtype=torch.double)\n",
    "# torch.autograd.gradcheck(Conv2D.apply, (X, weight))\n",
    "\n",
    "weight = torch.rand(5, 3, 3, 3, requires_grad=True, dtype=torch.double)\n",
    "X = torch.rand(2, 3, 4, 4, requires_grad=True, dtype=torch.double)\n",
    "torch.autograd.gradcheck(FusedConvBN2DFunction.apply, (X, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a36c35c6-0902-46dc-a4f8-d661c0248008",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n",
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n",
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.349959\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.183864\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.160671\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.075611\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.076669\n",
      "\n",
      "Test set: Average loss: 0.0557, Accuracy: 9804/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0559, Accuracy: 9804/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0553, Accuracy: 9805/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0558, Accuracy: 9805/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0557, Accuracy: 9805/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0558, Accuracy: 9804/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0556, Accuracy: 9805/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0559, Accuracy: 9804/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0558, Accuracy: 9804/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0558, Accuracy: 9804/10000 (98%)\n",
      "\n",
      "4.433199238777161\n"
     ]
    }
   ],
   "source": [
    "train_kwargs = {'batch_size': 128}\n",
    "test_kwargs = {'batch_size': 128}\n",
    "\n",
    "accel_kwargs = {'num_workers': 1,\n",
    "                'persistent_workers': True,\n",
    "               'pin_memory': True,\n",
    "               'shuffle': True}\n",
    "train_kwargs.update(accel_kwargs)\n",
    "test_kwargs.update(accel_kwargs)\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "dataset1 = datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.MNIST('./data', train=False,\n",
    "                   transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, drop_last =True, **test_kwargs)\n",
    "\n",
    "model = Net().cuda()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.1)\n",
    "\n",
    "# scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "\n",
    "# train(model, train_loader, optimizer, epoch, True)\n",
    "epoch = 0\n",
    "\n",
    "train(model, train_loader, optimizer, epoch)\n",
    "times = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    torch.cuda.synchronize()\n",
    "    start_epoch = time.time()\n",
    "    # train(model, train_loader, optimizer, epoch)\n",
    "    test(model, test_loader)\n",
    "    end_epoch = time.time()\n",
    "    elapsed = end_epoch - start_epoch\n",
    "    times.append(elapsed)\n",
    "\n",
    "avg_time = sum(times)/len(times)\n",
    "print(avg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928f13b9-50d1-462a-8dd6-35a8a875724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        # self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.convbn1 = FusedConvBN(1, 32, 3)\n",
    "        self.convbn2 = FusedConvBN(32, 64, 3)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.conv1(x)\n",
    "        # x = self.bn1(x)\n",
    "        x = self.convbn1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # x = self.conv2(x)\n",
    "        # x = self.bn2(x)\n",
    "        x = self.convbn2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d7b02-9711-4f4d-8edf-80b52edab018",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Net().cuda()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.1)\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "train(model, train_loader, optimizer, epoch)\n",
    "times = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    torch.cuda.synchronize()\n",
    "    start_epoch = time.time()\n",
    "    test(model, test_loader)\n",
    "    end_epoch = time.time()\n",
    "    elapsed = end_epoch - start_epoch\n",
    "    times.append(elapsed)\n",
    "\n",
    "avg_time = sum(times)/len(times)\n",
    "print(avg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43769b00-948a-4702-a260-af4d4ef6e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.device(\"cuda\"):\n",
    "    example_inputs = (torch.randn(128, 1, 28, 28),)\n",
    "    onnx_program = torch.onnx.export(model, example_inputs, \"model.onnx\", input_names=[\"x\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23465e9-4072-4d74-9c28-cfd0f0af5bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_program.save(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21d9c70-ef3a-4a0d-8bc1-1ceeb92cb83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    # model.eval()\n",
    "    # test_loss = 0\n",
    "    # correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            onnxruntime_input = {input_arg.name: input_value for input_arg, input_value in zip(ort_session.get_inputs(), onnx_inputs)}\n",
    "            \n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c35f5fd-a801-463e-9cd4-e5f06f908f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af416f12-a0b1-4ef6-99ad-72169f5fa9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "import tensorrt\n",
    "\n",
    "test_dataset = iter(test_loader)\n",
    "\n",
    "session = onnxruntime.InferenceSession(\"model.onnx\", providers=['TensorrtExecutionProvider','CUDAExecutionProvider'])\n",
    "\n",
    "start_epoch = time.time()\n",
    "for i in range(len(test_dataset)-1):\n",
    "    data = np.float32(next(test_dataset)[0])\n",
    "    outputs = session.run([], {'x':data})[0]\n",
    "    \n",
    "end_epoch = time.time()\n",
    "elapsed = end_epoch - start_epoch\n",
    "    \n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867a6d2-70e8-4f53-b0c2-2cc18e990475",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "start_epoch = time.time()\n",
    "for x, _ in test_loader:\n",
    "    x = x.cuda(non_blocking=True)\n",
    "\n",
    "    io_binding = session.io_binding()\n",
    "    io_binding.bind_input(\n",
    "        name=input_name,\n",
    "        device_type='cuda',\n",
    "        device_id=0,\n",
    "        element_type=np.float32,\n",
    "        shape=tuple(x.shape),\n",
    "        buffer_ptr=x.data_ptr()\n",
    "    )\n",
    "    io_binding.bind_output(output_name, 'cuda')\n",
    "    session.run_with_iobinding(io_binding)\n",
    "\n",
    "end_epoch = time.time()\n",
    "elapsed = end_epoch - start_epoch\n",
    "    \n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d6b3c1-400c-43a5-8008-0cf7dfc90678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
